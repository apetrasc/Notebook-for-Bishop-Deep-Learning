{"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9413607,"sourceType":"datasetVersion","datasetId":5716814}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 概要\n自己教師あり学習を用いて事前学習を行い，得られた表現をLinear probingで評価してみたノート。 Kaggle のT4を2台使うために適宜並列化してあります。 注意事項として、並列化に伴い、モデルを保存・ロードする際にやや工夫が必要になることがあります。特に、学習の直前で\n```\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel = model.to(device)\n```\nを実行すること、そしてモデルを読み込む際は\n```\nimport matplotlib as plt\nstate_dict = torch.load(model_path, map_location=device, weights_only=True)\nfrom collections import OrderedDict\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k.replace('module.', '')  # module.のプレフィックスを削除\n    new_state_dict[name] = v\n\nmodel.load_state_dict(new_state_dict)\n```\nによって'module.'を削除しなければならないということが挙げられます。\nTransformerは並列計算に特化した手法である（Bishop et al, 2023）ため、並列計算のメソッドを書き加えておくことは重要です。Kaggle無料版の計算資源をフル活用したCidar10用Tutorialコードとなっています。\nEpochは、20～30程度では暖機運転の途中となりほとんど意味を成しません。Epochは100以上を強く推奨します。元のノートブックでは事前学習にEpoch2000、分類器の学習にEpoch100回を使用していました。\n\n","metadata":{"id":"EUU-McVcFGJ0"}},{"cell_type":"markdown","source":"## 必要パッケージのインストール","metadata":{}},{"cell_type":"code","source":"!pip install einops","metadata":{"id":"rteXenmbw49z","outputId":"a2d5a57c-668f-4748-cb5e-8ad13692daf3","execution":{"iopub.status.busy":"2024-09-18T15:59:03.001637Z","iopub.execute_input":"2024-09-18T15:59:03.002326Z","iopub.status.idle":"2024-09-18T15:59:15.614972Z","shell.execute_reply.started":"2024-09-18T15:59:03.002285Z","shell.execute_reply":"2024-09-18T15:59:15.613941Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.8.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torchvision import transforms\nfrom tqdm import tqdm_notebook as tqdm\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch.optim as optim\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\nimport math\n#学習データ,テストデータのpath\nx_train_path='/kaggle/input/cifar10/data/x_train.npy'\nt_train_path='/kaggle/input/cifar10/data/t_train.npy'\nx_test_path='/kaggle/input/cifar10/data/x_test.npy'\n\nx_train = np.load(x_train_path)\nt_train = np.load(t_train_path)\nx_test = np.load(x_test_path)\n\nclass train_dataset(torch.utils.data.Dataset):\n    def __init__(self, x_train, t_train):\n        data = x_train.astype('float32')\n        self.x_train = []\n        for i in range(data.shape[0]):\n            self.x_train.append(Image.fromarray(np.uint8(data[i])))\n        self.t_train = t_train\n        self.transform = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.x_train)\n\n    def __getitem__(self, idx):\n        return self.transform(self.x_train[idx]), torch.tensor(t_train[idx], dtype=torch.long)\n\nclass test_dataset(torch.utils.data.Dataset):\n    def __init__(self, x_test):\n        data = x_test.astype('float32')\n        self.x_test = []\n        for i in range(data.shape[0]):\n            self.x_test.append(Image.fromarray(np.uint8(data[i])))\n        self.transform = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.x_test)\n\n    def __getitem__(self, idx):\n        return self.transform(self.x_test[idx])\n\ntrainval_data = train_dataset(x_train, t_train)\ntest_data = test_dataset(x_test)","metadata":{"id":"LNpUF5xOJ8bG","execution":{"iopub.status.busy":"2024-09-18T15:59:15.617339Z","iopub.execute_input":"2024-09-18T15:59:15.617688Z","iopub.status.idle":"2024-09-18T15:59:17.817875Z","shell.execute_reply.started":"2024-09-18T15:59:15.617650Z","shell.execute_reply":"2024-09-18T15:59:17.816822Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## データローダの準備  ","metadata":{"id":"fSqA6Ni3MDSX"}},{"cell_type":"code","source":"val_size = 3000\ntrain_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n\ntrain_transform = transforms.Compose(\n    [transforms.RandomCrop(32, padding=4),\n     transforms.RandomHorizontalFlip(),\n     transforms.ToTensor(),\n     transforms.Normalize(0.5, 0.5)]\n)\nvalid_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize(0.5, 0.5)]\n)\n\nbatch_size = 128\n\ndataloader_train = torch.utils.data.DataLoader(\n    train_data,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndataloader_valid = torch.utils.data.DataLoader(\n    valid_data,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndataloader_test = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=batch_size,\n    shuffle=False\n)","metadata":{"id":"63ODMwChMEy_","execution":{"iopub.status.busy":"2024-09-18T15:59:17.819253Z","iopub.execute_input":"2024-09-18T15:59:17.819660Z","iopub.status.idle":"2024-09-18T15:59:17.979995Z","shell.execute_reply.started":"2024-09-18T15:59:17.819614Z","shell.execute_reply":"2024-09-18T15:59:17.978999Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"## その他必要な関数やモデルなど","metadata":{}},{"cell_type":"code","source":"def fix_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n\nfix_seed(seed=42)\n\n\ndef random_indexes(size):\n    \"\"\"\n    パッチをランダムに並べ替えるためのindexを生成する関数．\n\n    Argument\n    --------\n    size : int\n        入力されるパッチの数（系列長Nと同じ値）．\n    \"\"\"\n    forward_indexes = np.arange(size)  # 0からsizeまでを並べた配列を作成\n    np.random.shuffle(forward_indexes)  # 生成した配列をシャッフルすることで，パッチの順番をランダムに決定\n    backward_indexes = np.argsort(forward_indexes)  # 並べ替えたパッチをもとの順番に戻すためのidx\n\n    return forward_indexes, backward_indexes\n\n\ndef take_indexes(sequences, indexes):\n    \"\"\"\n    パッチを並べ替えるための関数．\n\n    Argument\n    --------\n    sequences : torch.Tensor\n        入力画像をパッチ分割したデータ．(B, N, dim)の形状をしている．\n    indexes : np.ndarray\n        並べ替えるために利用するindex．\n        random_indexesで生成したforward_indexesかbackward_indexesが入ることが想定されている．\n    \"\"\"\n    return torch.gather(sequences, dim=1, index=indexes.unsqueeze(2).repeat(1, 1, sequences.shape[-1]))\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads, dim_head, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n        dim : int\n            入力データの次元数．埋め込み次元数と一致する．\n        heads : int\n            ヘッドの数．\n        dim_head : int\n            各ヘッドのデータの次元数．\n        dropout : float\n            Dropoutの確率(default=0.)．\n        \"\"\"\n        super().__init__()\n\n        self.dim = dim\n        self.dim_head = dim_head\n        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n\n        self.heads = heads\n        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n\n        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n        self.dropout = nn.Dropout(dropout)\n\n        # Q, K, Vに変換するための全結合層\n        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n\n        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n        self.to_out = nn.Sequential(\n            nn.Linear(in_features=inner_dim, out_features=dim),\n            nn.Dropout(dropout),\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        \"\"\"\n        B: バッチサイズ\n        N: 系列長\n        D: データの次元数(dim)\n        \"\"\"\n        B, N, D = x.size()\n\n        # 入力データをQ, K, Vに変換する\n        # (B, N, dim) -> (B, N, inner_dim)\n        q = self.to_q(x)\n        k = self.to_k(x)\n        v = self.to_v(x)\n\n        # Q, K, Vをヘッドに分割する\n        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n\n        # QK^T / sqrt(d_k)を計算する\n        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n\n        # ソフトマックス関数でスコアを算出し，Dropoutをする\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        # softmax(QK^T / sqrt(d_k))Vを計算する\n        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n        out = torch.matmul(attn ,v)\n\n        # もとの形に戻す\n        # (B, heads, N, dim_head) -> (B, N, dim)\n        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n\n        # 次元が違っていればもとに戻して出力\n        # 表現の可視化のためにattention mapも返すようにしておく\n        return self.to_out(out), attn\n\n\nclass FFN(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n        dim : int\n            入力データの次元数．\n        hidden_dim : int\n            隠れ層の次元．\n        dropout : float\n            各全結合層の後のDropoutの確率(default=0.)．\n        \"\"\"\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(in_features=dim, out_features=hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(in_features=hidden_dim, out_features=dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        \"\"\"\n        (B, D) -> (B, D)\n        B: バッチサイズ\n        D: 次元数\n        \"\"\"\n        return self.net(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n        \"\"\"\n        TransformerのEncoder Blockの実装．\n\n        Arguments\n        ---------\n        dim : int\n            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n        heads : int\n            Multi-Head Attentionのヘッドの数．\n        dim_head : int\n            Multi-Head Attentionの各ヘッドの次元数．\n        mlp_dim : int\n            Feed-Forward Networkの隠れ層の次元数．\n        dropout : float\n            Droptou層の確率p．\n        \"\"\"\n        super().__init__()\n\n        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n        self.attn = Attention(dim, heads, dim_head, dropout)\n        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n        self.ffn = FFN(dim, mlp_dim, dropout)\n\n    def forward(self, x, return_attn=False):\n        \"\"\"\n        x: (B, N, dim)\n        B: バッチサイズ\n        N: 系列長\n        dim: 埋め込み次元\n        \"\"\"\n        y, attn = self.attn(self.attn_ln(x))\n        if return_attn:  # attention mapを返す（attention mapの可視化に利用）\n            return attn\n        x = y + x\n        out = self.ffn(self.ffn_ln(x)) + x\n\n        return out\n\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        \"\"\"\n        入力画像をパッチごとに埋め込むための層．\n\n        Arguments\n        ---------\n        image_size : Tuple[int]\n            入力画像のサイズ．\n        patch_size : Tuple[int]\n            各パッチのサイズ．\n        in_channels : int\n            入力画像のチャネル数．\n        embed_dim : int\n            埋め込み後の次元数．\n        \"\"\"\n        super().__init__()\n\n        image_height, image_width = image_size\n        patch_height, patch_width = patch_size\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n        )\n\n    def forward(self, x):\n        \"\"\"\n        B: バッチサイズ\n        C: 入力画像のチャネル数\n        H: 入力画像の高さ\n        W: 入力画像の幅\n        \"\"\"\n        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)\n\n\nclass PatchShuffle(nn.Module):\n    def __init__(self, ratio):\n        # ratio: Encoderに入力しないパッチの割合\n        super().__init__()\n        self.ratio = ratio\n\n    def forward(self, patches):\n        \"\"\"\n        B: バッチサイズ\n        N: 系列長（＝パッチの数）\n        dim: 次元数（＝埋め込みの次元数）\n        \"\"\"\n        B, N, dim = patches.shape\n        remain_N = int(N * (1 - self.ratio))  # Encoderに入力するパッチの数\n\n        indexes = [random_indexes(N) for _ in range(B)]  # バッチごとに異なる順番のindexを作る\n        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # バッチを並べ替えるときのidx (B, N)\n        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # 並べ替えたパッチをもとの順番に戻すためのidx  (B, N)\n\n        patches = take_indexes(patches, forward_indexes)  # パッチを並べ替える\n        patches = patches[:, :remain_N, :]  # Encoderに入力するパッチを抽出\n\n        return patches, forward_indexes, backward_indexes\n\n\nclass MAE_Encoder(torch.nn.Module):\n    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=12,\n                 heads=3, dim_head=64, mlp_dim=192, mask_ratio=0.75, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n\n        image_size : List[int]\n            入力画像の大きさ．\n        patch_size : List[int]\n            各パッチの大きさ．\n        emb_dim : int\n            データを埋め込む次元の数．\n        num_layer : int\n            Encoderに含まれるBlockの数．\n        heads : int\n            Multi-Head Attentionのヘッドの数．\n        dim_head : int\n            Multi-Head Attentionの各ヘッドの次元数．\n        mlp_dim : int\n            Feed-Forward Networkの隠れ層の次元数．\n        mask_ratio : float\n            入力パッチのマスクする割合．\n        dropout : float\n            ドロップアウトの確率．\n        \"\"\"\n        super().__init__()\n        img_height, img_width = image_size\n        patch_height, patch_width = patch_size\n        num_patches = (img_height // patch_height) * (img_width // patch_width)\n\n        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, emb_dim))  # class tokenの初期化\n        self.pos_embedding = torch.nn.Parameter(torch.randn(1, num_patches, emb_dim))  # positional embedding（学習可能にしている）\n        self.shuffle = PatchShuffle(mask_ratio)\n\n        # 入力画像をパッチに分割する\n        self.patchify = PatchEmbedding(image_size, patch_size, 3, emb_dim)\n\n        # Encoder（Blockを重ねる）\n        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n\n        self.layer_norm = nn.LayerNorm(emb_dim)\n\n        self.init_weight()\n\n    def init_weight(self):\n        torch.nn.init.normal_(self.cls_token, std=0.02)\n        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n\n    def forward(self, img):\n        # 1. 入力画像をパッチに分割して，positional embeddingする\n        patches = self.patchify(img)\n        patches = patches + self.pos_embedding\n\n        # 2. 分割したパッチをランダムに並べ替えて，必要なパッチのみ得る\n        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n\n        # class tokenを結合\n        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)\n\n        # 3. Encoderで入力データを処理する\n        features = self.layer_norm(self.transformer(patches))\n\n        return features, backward_indexes\n\n\nclass MAE_Decoder(nn.Module):\n    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=4,\n                 heads=3, dim_head=64, mlp_dim=192, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n\n        image_size : List[int]\n            入力画像の大きさ．\n        patch_size : List[int]\n            各パッチの大きさ．\n        emb_dim : int\n            データを埋め込む次元の数．\n        num_layer : int\n            Decoderに含まれるBlockの数．\n        heads : int\n            Multi-Head Attentionのヘッドの数．\n        dim_head : int\n            Multi-Head Attentionの各ヘッドの次元数．\n        mlp_dim : int\n            Feed-Forward Networkの隠れ層の次元数．\n        dropout : float\n            ドロップアウトの確率．\n        \"\"\"\n        super().__init__()\n        img_height, img_width = image_size\n        patch_height, patch_width = patch_size\n        num_patches = (img_height // patch_height) * (img_width // patch_width)\n\n        self.mask_token = torch.nn.Parameter(torch.rand(1, 1, emb_dim))\n        self.pos_embedding = torch.nn.Parameter(torch.rand(1, num_patches+1, emb_dim))\n\n        # Decoder(Blockを重ねる）\n        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n\n        # 埋め込みされた表現から画像を復元するためのhead\n        self.head = torch.nn.Linear(emb_dim, 3 * patch_height * patch_width)\n        # (B, N, dim)から(B, C, H, W)にreshapeするためのインスタンス\n        self.patch2img = Rearrange(\"b (h w) (c p1 p2) -> b c (h p1) (w p2)\", p1=patch_height, p2=patch_width, h=img_height // patch_height)\n\n        self.init_weight()\n\n    def init_weight(self):\n        torch.nn.init.normal_(self.mask_token, std=0.02)\n        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n\n    def forward(self, features, backward_indexes):\n        # 系列長\n        T = features.shape[1]\n\n        # class tokenがある分backward_indexesの最初に0を追加する\n        # .toはデバイスの変更でよく利用するが，tensorを渡すことでdtypeを変えることができる\n        backward_indexes = torch.cat([torch.zeros(backward_indexes.shape[0], 1).to(backward_indexes), backward_indexes+1], dim=1)\n\n        # 1. mask_tokenを結合して並べ替える．\n        # (B, N*(1-mask_ratio)+1, dim) -> (B, N+1, dim)\n        features = torch.cat([features, self.mask_token.repeat(features.shape[0], backward_indexes.shape[1] - features.shape[1], 1)], dim=1)\n        features = take_indexes(features, backward_indexes)\n        features = features + self.pos_embedding\n\n        features = self.transformer(features)\n\n        # class tokenを除去する\n        # (B, N+1, dim) -> (B, N, dim)\n        features = features[:, 1:, :]\n\n        # 2. 画像を再構成する．\n        # (B, N, dim) -> (B, N, 3 * patch_height * patch_width)\n        patches = self.head(features)\n\n        # MAEではマスクした部分でのみ損失関数を計算するため，maskも一緒に返す\n        mask = torch.zeros_like(patches)\n        mask[:, T-1:] = 1  # cls tokenを含めていた分ずらしている\n        mask = take_indexes(mask, backward_indexes[:, 1:] - 1)\n\n        img = self.patch2img(patches)\n        mask = self.patch2img(mask)\n\n        return img, mask\n\n\nclass MAE_ViT(torch.nn.Module):\n    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192,\n                 enc_layers=12, enc_heads=3, enc_dim_head=64, enc_mlp_dim=768,\n                 dec_layers=4, dec_heads=3, dec_dim_head=64, dec_mlp_dim=768,\n                 mask_ratio=0.75, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n        image_size : List[int]\n            入力画像の大きさ．\n        patch_size : List[int]\n            各パッチの大きさ．\n        emb_dim : int\n            データを埋め込む次元の数．\n        {enc/dec}_layers : int\n            Encoder / Decoderに含まれるBlockの数．\n        {enc/dec}_heads : int\n            Encoder / DecoderのMulti-Head Attentionのヘッドの数．\n        {enc/dec}_dim_head : int\n            Encoder / DecoderのMulti-Head Attentionの各ヘッドの次元数．\n        {enc/dec}_mlp_dim : int\n            Encoder / DecoderのFeed-Forward Networkの隠れ層の次元数．\n        mask_ratio : float\n            入力パッチのマスクする割合．\n        dropout : float\n            ドロップアウトの確率．\n        \"\"\"\n        super().__init__()\n\n        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, enc_layers,\n                                   enc_heads, enc_dim_head, enc_mlp_dim, mask_ratio, dropout)\n        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, dec_layers,\n                                   dec_heads, dec_dim_head, dec_mlp_dim, dropout)\n\n    def forward(self, img):\n        features, backward_indexes = self.encoder(img)\n        rec_img, mask = self.decoder(features, backward_indexes)\n        return rec_img, mask\n\n    def get_last_selfattention(self, x):\n        patches = self.encoder.patchify(x)\n        patches = patches + self.encoder.pos_embedding\n\n        patches = torch.cat([self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n        for i, block in enumerate(self.encoder.transformer):\n            if i < len(self.encoder.transformer) - 1:\n                patches = block(patches)\n            else:\n                return block(patches, return_attn=True)\n\nconfig = {\n    \"image_size\": [32, 32],\n    \"patch_size\": [2, 2],\n    \"emb_dim\": 192,\n    \"enc_layers\": 12,\n    \"enc_heads\": 3,\n    \"enc_dim_head\": 64,\n    \"enc_mlp_dim\": 192,\n    \"dec_layers\": 4,\n    \"dec_heads\": 3,\n    \"dec_dim_head\": 64,\n    \"dec_mlp_dim\": 192,\n    \"mask_ratio\": 0.75,\n    \"dropout\": 0.\n}\n# cosine scheduler\nclass CosineScheduler:\n    def __init__(self, epochs, lr, warmup_length=5):\n        \"\"\"\n        Arguments\n        ---------\n        epochs : int\n            学習のエポック数．\n        lr : float\n            学習率．\n        warmup_length : int\n            warmupを適用するエポック数．\n        \"\"\"\n        self.epochs = epochs\n        self.lr = lr\n        self.warmup = warmup_length\n\n    def __call__(self, epoch):\n        \"\"\"\n        Arguments\n        ---------\n        epoch : int\n            現在のエポック数．\n        \"\"\"\n        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n        progress = np.clip(progress, 0.0, 1.0)\n        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n\n        if self.warmup:\n            lr = lr * min(1., (epoch+1) / self.warmup)\n\n        return lr\ndef set_lr(lr, optimizer):\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = MAE_ViT(**config).to(device)\nepochs = 200\nlr = 0.0024\nbatch_size = 512\nstep_count = 0\nwarmup_length = 20\noptimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.05)\nscheduler = CosineScheduler(epochs, lr, warmup_length)","metadata":{"id":"TzlJ4q1uKagF","execution":{"iopub.status.busy":"2024-09-18T15:59:17.983901Z","iopub.execute_input":"2024-09-18T15:59:17.984252Z","iopub.status.idle":"2024-09-18T15:59:18.110879Z","shell.execute_reply.started":"2024-09-18T15:59:17.984201Z","shell.execute_reply":"2024-09-18T15:59:18.110163Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"## 事前学習（自己教師あり学習）の実行","metadata":{"id":"uR8uNlkCxo3d"}},{"cell_type":"code","source":"# DataParallelを使用して複数GPUで並列処理\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\n\n# モデルをデバイスに送る\nmodel = model.to(device)\n\n# 訓練ループ\nfor epoch in range(epochs):\n    # 学習率の更新\n    new_lr = scheduler(epoch)\n    set_lr(new_lr, optimizer)\n\n    total_train_loss = 0.\n    total_valid_loss = 0.\n\n    # モデルの訓練\n    for x, _ in dataloader_train:\n        step_count += 1\n        model.train()\n\n        x = x.to(device)\n        rec_img, mask = model(x)  # ここで並列処理\n\n        train_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n        train_loss.backward()\n\n        if step_count % 8 == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        total_train_loss += train_loss.item()\n\n    # モデルの評価\n    with torch.no_grad():\n        for x, _ in dataloader_valid:\n            model.eval()\n            x = x.to(device)\n            rec_img, mask = model(x)\n            valid_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n            total_valid_loss += valid_loss.item()\n\n    print(f\"Epoch[{epoch+1} / {epochs}] Train Loss: {total_train_loss/len(dataloader_train):.4f} Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\")\n\n# モデルの保存\nmodel_path=\"/kaggle/working/MAE_pretrain_params.pth\"\ntorch.save(model.state_dict(), model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T15:59:18.111959Z","iopub.execute_input":"2024-09-18T15:59:18.112279Z","iopub.status.idle":"2024-09-18T19:36:37.269523Z","shell.execute_reply.started":"2024-09-18T15:59:18.112247Z","shell.execute_reply":"2024-09-18T19:36:37.268450Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Using 2 GPUs!\nEpoch[1 / 200] Train Loss: 0.0654 Valid Loss: 0.0474\nEpoch[2 / 200] Train Loss: 0.0475 Valid Loss: 0.0459\nEpoch[3 / 200] Train Loss: 0.0484 Valid Loss: 0.0440\nEpoch[4 / 200] Train Loss: 0.0461 Valid Loss: 0.0432\nEpoch[5 / 200] Train Loss: 0.0447 Valid Loss: 0.0407\nEpoch[6 / 200] Train Loss: 0.0449 Valid Loss: 0.0430\nEpoch[7 / 200] Train Loss: 0.0422 Valid Loss: 0.0387\nEpoch[8 / 200] Train Loss: 0.0372 Valid Loss: 0.0360\nEpoch[9 / 200] Train Loss: 0.0358 Valid Loss: 0.0356\nEpoch[10 / 200] Train Loss: 0.0362 Valid Loss: 0.0340\nEpoch[11 / 200] Train Loss: 0.0322 Valid Loss: 0.0324\nEpoch[12 / 200] Train Loss: 0.0324 Valid Loss: 0.0299\nEpoch[13 / 200] Train Loss: 0.0307 Valid Loss: 0.0311\nEpoch[14 / 200] Train Loss: 0.0299 Valid Loss: 0.0287\nEpoch[15 / 200] Train Loss: 0.0298 Valid Loss: 0.0278\nEpoch[16 / 200] Train Loss: 0.0277 Valid Loss: 0.0281\nEpoch[17 / 200] Train Loss: 0.0291 Valid Loss: 0.0262\nEpoch[18 / 200] Train Loss: 0.0255 Valid Loss: 0.0252\nEpoch[19 / 200] Train Loss: 0.0248 Valid Loss: 0.0243\nEpoch[20 / 200] Train Loss: 0.0247 Valid Loss: 0.0232\nEpoch[21 / 200] Train Loss: 0.0230 Valid Loss: 0.0236\nEpoch[22 / 200] Train Loss: 0.0226 Valid Loss: 0.0221\nEpoch[23 / 200] Train Loss: 0.0206 Valid Loss: 0.0201\nEpoch[24 / 200] Train Loss: 0.0210 Valid Loss: 0.0193\nEpoch[25 / 200] Train Loss: 0.0184 Valid Loss: 0.0192\nEpoch[26 / 200] Train Loss: 0.0175 Valid Loss: 0.0168\nEpoch[27 / 200] Train Loss: 0.0164 Valid Loss: 0.0160\nEpoch[28 / 200] Train Loss: 0.0158 Valid Loss: 0.0154\nEpoch[29 / 200] Train Loss: 0.0149 Valid Loss: 0.0145\nEpoch[30 / 200] Train Loss: 0.0145 Valid Loss: 0.0138\nEpoch[31 / 200] Train Loss: 0.0138 Valid Loss: 0.0135\nEpoch[32 / 200] Train Loss: 0.0131 Valid Loss: 0.0131\nEpoch[33 / 200] Train Loss: 0.0126 Valid Loss: 0.0124\nEpoch[34 / 200] Train Loss: 0.0122 Valid Loss: 0.0119\nEpoch[35 / 200] Train Loss: 0.0119 Valid Loss: 0.0117\nEpoch[36 / 200] Train Loss: 0.0117 Valid Loss: 0.0113\nEpoch[37 / 200] Train Loss: 0.0114 Valid Loss: 0.0113\nEpoch[38 / 200] Train Loss: 0.0112 Valid Loss: 0.0111\nEpoch[39 / 200] Train Loss: 0.0110 Valid Loss: 0.0117\nEpoch[40 / 200] Train Loss: 0.0108 Valid Loss: 0.0107\nEpoch[41 / 200] Train Loss: 0.0108 Valid Loss: 0.0108\nEpoch[42 / 200] Train Loss: 0.0105 Valid Loss: 0.0110\nEpoch[43 / 200] Train Loss: 0.0104 Valid Loss: 0.0103\nEpoch[44 / 200] Train Loss: 0.0104 Valid Loss: 0.0103\nEpoch[45 / 200] Train Loss: 0.0102 Valid Loss: 0.0103\nEpoch[46 / 200] Train Loss: 0.0101 Valid Loss: 0.0100\nEpoch[47 / 200] Train Loss: 0.0100 Valid Loss: 0.0102\nEpoch[48 / 200] Train Loss: 0.0099 Valid Loss: 0.0102\nEpoch[49 / 200] Train Loss: 0.0100 Valid Loss: 0.0102\nEpoch[50 / 200] Train Loss: 0.0098 Valid Loss: 0.0099\nEpoch[51 / 200] Train Loss: 0.0098 Valid Loss: 0.0097\nEpoch[52 / 200] Train Loss: 0.0097 Valid Loss: 0.0096\nEpoch[53 / 200] Train Loss: 0.0096 Valid Loss: 0.0098\nEpoch[54 / 200] Train Loss: 0.0096 Valid Loss: 0.0098\nEpoch[55 / 200] Train Loss: 0.0095 Valid Loss: 0.0095\nEpoch[56 / 200] Train Loss: 0.0095 Valid Loss: 0.0094\nEpoch[57 / 200] Train Loss: 0.0094 Valid Loss: 0.0099\nEpoch[58 / 200] Train Loss: 0.0095 Valid Loss: 0.0095\nEpoch[59 / 200] Train Loss: 0.0094 Valid Loss: 0.0094\nEpoch[60 / 200] Train Loss: 0.0093 Valid Loss: 0.0093\nEpoch[61 / 200] Train Loss: 0.0093 Valid Loss: 0.0092\nEpoch[62 / 200] Train Loss: 0.0093 Valid Loss: 0.0093\nEpoch[63 / 200] Train Loss: 0.0092 Valid Loss: 0.0095\nEpoch[64 / 200] Train Loss: 0.0092 Valid Loss: 0.0092\nEpoch[65 / 200] Train Loss: 0.0092 Valid Loss: 0.0092\nEpoch[66 / 200] Train Loss: 0.0092 Valid Loss: 0.0091\nEpoch[67 / 200] Train Loss: 0.0091 Valid Loss: 0.0093\nEpoch[68 / 200] Train Loss: 0.0091 Valid Loss: 0.0091\nEpoch[69 / 200] Train Loss: 0.0091 Valid Loss: 0.0091\nEpoch[70 / 200] Train Loss: 0.0090 Valid Loss: 0.0094\nEpoch[71 / 200] Train Loss: 0.0090 Valid Loss: 0.0091\nEpoch[72 / 200] Train Loss: 0.0090 Valid Loss: 0.0089\nEpoch[73 / 200] Train Loss: 0.0089 Valid Loss: 0.0090\nEpoch[74 / 200] Train Loss: 0.0090 Valid Loss: 0.0090\nEpoch[75 / 200] Train Loss: 0.0089 Valid Loss: 0.0090\nEpoch[76 / 200] Train Loss: 0.0089 Valid Loss: 0.0090\nEpoch[77 / 200] Train Loss: 0.0089 Valid Loss: 0.0089\nEpoch[78 / 200] Train Loss: 0.0088 Valid Loss: 0.0089\nEpoch[79 / 200] Train Loss: 0.0088 Valid Loss: 0.0089\nEpoch[80 / 200] Train Loss: 0.0088 Valid Loss: 0.0088\nEpoch[81 / 200] Train Loss: 0.0088 Valid Loss: 0.0089\nEpoch[82 / 200] Train Loss: 0.0088 Valid Loss: 0.0088\nEpoch[83 / 200] Train Loss: 0.0088 Valid Loss: 0.0088\nEpoch[84 / 200] Train Loss: 0.0087 Valid Loss: 0.0088\nEpoch[85 / 200] Train Loss: 0.0088 Valid Loss: 0.0088\nEpoch[86 / 200] Train Loss: 0.0087 Valid Loss: 0.0088\nEpoch[87 / 200] Train Loss: 0.0087 Valid Loss: 0.0088\nEpoch[88 / 200] Train Loss: 0.0087 Valid Loss: 0.0087\nEpoch[89 / 200] Train Loss: 0.0087 Valid Loss: 0.0089\nEpoch[90 / 200] Train Loss: 0.0087 Valid Loss: 0.0086\nEpoch[91 / 200] Train Loss: 0.0086 Valid Loss: 0.0088\nEpoch[92 / 200] Train Loss: 0.0086 Valid Loss: 0.0087\nEpoch[93 / 200] Train Loss: 0.0086 Valid Loss: 0.0088\nEpoch[94 / 200] Train Loss: 0.0086 Valid Loss: 0.0087\nEpoch[95 / 200] Train Loss: 0.0086 Valid Loss: 0.0087\nEpoch[96 / 200] Train Loss: 0.0086 Valid Loss: 0.0088\nEpoch[97 / 200] Train Loss: 0.0086 Valid Loss: 0.0086\nEpoch[98 / 200] Train Loss: 0.0085 Valid Loss: 0.0085\nEpoch[99 / 200] Train Loss: 0.0085 Valid Loss: 0.0086\nEpoch[100 / 200] Train Loss: 0.0085 Valid Loss: 0.0086\nEpoch[101 / 200] Train Loss: 0.0085 Valid Loss: 0.0085\nEpoch[102 / 200] Train Loss: 0.0085 Valid Loss: 0.0087\nEpoch[103 / 200] Train Loss: 0.0085 Valid Loss: 0.0086\nEpoch[104 / 200] Train Loss: 0.0084 Valid Loss: 0.0087\nEpoch[105 / 200] Train Loss: 0.0084 Valid Loss: 0.0087\nEpoch[106 / 200] Train Loss: 0.0084 Valid Loss: 0.0085\nEpoch[107 / 200] Train Loss: 0.0084 Valid Loss: 0.0085\nEpoch[108 / 200] Train Loss: 0.0084 Valid Loss: 0.0085\nEpoch[109 / 200] Train Loss: 0.0084 Valid Loss: 0.0085\nEpoch[110 / 200] Train Loss: 0.0084 Valid Loss: 0.0085\nEpoch[111 / 200] Train Loss: 0.0084 Valid Loss: 0.0084\nEpoch[112 / 200] Train Loss: 0.0084 Valid Loss: 0.0086\nEpoch[113 / 200] Train Loss: 0.0083 Valid Loss: 0.0085\nEpoch[114 / 200] Train Loss: 0.0083 Valid Loss: 0.0085\nEpoch[115 / 200] Train Loss: 0.0083 Valid Loss: 0.0085\nEpoch[116 / 200] Train Loss: 0.0083 Valid Loss: 0.0084\nEpoch[117 / 200] Train Loss: 0.0083 Valid Loss: 0.0084\nEpoch[118 / 200] Train Loss: 0.0083 Valid Loss: 0.0084\nEpoch[119 / 200] Train Loss: 0.0083 Valid Loss: 0.0085\nEpoch[120 / 200] Train Loss: 0.0083 Valid Loss: 0.0084\nEpoch[121 / 200] Train Loss: 0.0082 Valid Loss: 0.0084\nEpoch[122 / 200] Train Loss: 0.0083 Valid Loss: 0.0084\nEpoch[123 / 200] Train Loss: 0.0082 Valid Loss: 0.0083\nEpoch[124 / 200] Train Loss: 0.0082 Valid Loss: 0.0083\nEpoch[125 / 200] Train Loss: 0.0082 Valid Loss: 0.0083\nEpoch[126 / 200] Train Loss: 0.0082 Valid Loss: 0.0083\nEpoch[127 / 200] Train Loss: 0.0082 Valid Loss: 0.0083\nEpoch[128 / 200] Train Loss: 0.0082 Valid Loss: 0.0084\nEpoch[129 / 200] Train Loss: 0.0082 Valid Loss: 0.0083\nEpoch[130 / 200] Train Loss: 0.0082 Valid Loss: 0.0083\nEpoch[131 / 200] Train Loss: 0.0081 Valid Loss: 0.0082\nEpoch[132 / 200] Train Loss: 0.0081 Valid Loss: 0.0083\nEpoch[133 / 200] Train Loss: 0.0081 Valid Loss: 0.0082\nEpoch[134 / 200] Train Loss: 0.0081 Valid Loss: 0.0083\nEpoch[135 / 200] Train Loss: 0.0081 Valid Loss: 0.0082\nEpoch[136 / 200] Train Loss: 0.0081 Valid Loss: 0.0082\nEpoch[137 / 200] Train Loss: 0.0081 Valid Loss: 0.0082\nEpoch[138 / 200] Train Loss: 0.0081 Valid Loss: 0.0082\nEpoch[139 / 200] Train Loss: 0.0081 Valid Loss: 0.0083\nEpoch[140 / 200] Train Loss: 0.0080 Valid Loss: 0.0081\nEpoch[141 / 200] Train Loss: 0.0081 Valid Loss: 0.0083\nEpoch[142 / 200] Train Loss: 0.0081 Valid Loss: 0.0082\nEpoch[143 / 200] Train Loss: 0.0080 Valid Loss: 0.0082\nEpoch[144 / 200] Train Loss: 0.0080 Valid Loss: 0.0082\nEpoch[145 / 200] Train Loss: 0.0080 Valid Loss: 0.0082\nEpoch[146 / 200] Train Loss: 0.0080 Valid Loss: 0.0082\nEpoch[147 / 200] Train Loss: 0.0080 Valid Loss: 0.0081\nEpoch[148 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[149 / 200] Train Loss: 0.0080 Valid Loss: 0.0081\nEpoch[150 / 200] Train Loss: 0.0080 Valid Loss: 0.0081\nEpoch[151 / 200] Train Loss: 0.0080 Valid Loss: 0.0081\nEpoch[152 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[153 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[154 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[155 / 200] Train Loss: 0.0079 Valid Loss: 0.0082\nEpoch[156 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[157 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[158 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[159 / 200] Train Loss: 0.0079 Valid Loss: 0.0080\nEpoch[160 / 200] Train Loss: 0.0079 Valid Loss: 0.0080\nEpoch[161 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[162 / 200] Train Loss: 0.0079 Valid Loss: 0.0080\nEpoch[163 / 200] Train Loss: 0.0079 Valid Loss: 0.0081\nEpoch[164 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[165 / 200] Train Loss: 0.0079 Valid Loss: 0.0080\nEpoch[166 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[167 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[168 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[169 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[170 / 200] Train Loss: 0.0078 Valid Loss: 0.0081\nEpoch[171 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[172 / 200] Train Loss: 0.0078 Valid Loss: 0.0079\nEpoch[173 / 200] Train Loss: 0.0078 Valid Loss: 0.0079\nEpoch[174 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[175 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[176 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[177 / 200] Train Loss: 0.0078 Valid Loss: 0.0079\nEpoch[178 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[179 / 200] Train Loss: 0.0078 Valid Loss: 0.0079\nEpoch[180 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[181 / 200] Train Loss: 0.0078 Valid Loss: 0.0079\nEpoch[182 / 200] Train Loss: 0.0078 Valid Loss: 0.0079\nEpoch[183 / 200] Train Loss: 0.0078 Valid Loss: 0.0080\nEpoch[184 / 200] Train Loss: 0.0077 Valid Loss: 0.0080\nEpoch[185 / 200] Train Loss: 0.0077 Valid Loss: 0.0080\nEpoch[186 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[187 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[188 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[189 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[190 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[191 / 200] Train Loss: 0.0078 Valid Loss: 0.0078\nEpoch[192 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[193 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[194 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[195 / 200] Train Loss: 0.0077 Valid Loss: 0.0080\nEpoch[196 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[197 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[198 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[199 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\nEpoch[200 / 200] Train Loss: 0.0077 Valid Loss: 0.0079\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# モデルの定義\nmodel = MAE_ViT(**config).to(device)\n\n# 保存したstate_dictをロード\nstate_dict = torch.load(model_path, map_location=device, weights_only=True)\nfrom collections import OrderedDict\n\n# DataParallelの'module.'プレフィックスを削除\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k.replace('module.', '')  # 'module.'のプレフィックスを削除\n    new_state_dict[name] = v\n\n# ロードしたパラメータをモデルにセット\nmodel.load_state_dict(new_state_dict)\n\n# 評価モードに設定\nmodel.eval()\n\n# データを取得して推論を行う\nx, _ = next(iter(dataloader_valid))\nwith torch.no_grad():\n    rec_img, mask = model(x.to(device))\n\n# CPUに移して結果を確認\nx, rec_img, mask = x.to(\"cpu\"), rec_img.to(\"cpu\"), mask.to(\"cpu\")\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-18T19:36:37.270770Z","iopub.execute_input":"2024-09-18T19:36:37.271091Z","iopub.status.idle":"2024-09-18T19:36:37.549994Z","shell.execute_reply.started":"2024-09-18T19:36:37.271057Z","shell.execute_reply":"2024-09-18T19:36:37.549191Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"### Linear probing","metadata":{"id":"hHOBi4auxuPR"}},{"cell_type":"code","source":"val_size = 3000\ntrain_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n\ntrain_transform = transforms.Compose(\n    [transforms.RandomCrop(32, padding=4),\n     transforms.RandomHorizontalFlip(),\n     transforms.ToTensor(),\n     transforms.Normalize(0.5, 0.5)]\n)\nvalid_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize(0.5, 0.5)]\n)\n\nbatch_size = 128\n\ndataloader_train = torch.utils.data.DataLoader(\n    train_data,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndataloader_valid = torch.utils.data.DataLoader(\n    valid_data,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndataloader_test = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=batch_size,\n    shuffle=False\n)","metadata":{"id":"1GeuhPBryfQa","execution":{"iopub.status.busy":"2024-09-18T19:36:37.551030Z","iopub.execute_input":"2024-09-18T19:36:37.551307Z","iopub.status.idle":"2024-09-18T19:36:37.562784Z","shell.execute_reply.started":"2024-09-18T19:36:37.551276Z","shell.execute_reply":"2024-09-18T19:36:37.561856Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, encoder: MAE_Encoder, num_classes=10):\n        super().__init__()\n        self.cls_token = encoder.cls_token\n        self.pos_embedding = encoder.pos_embedding\n        self.patchify = encoder.patchify\n        self.transformer = encoder.transformer\n        self.layer_norm = encoder.layer_norm\n        self.head = nn.Linear(self.pos_embedding.shape[-1], num_classes)\n\n    def forward(self, img):\n        patches = self.patchify(img)\n        patches = patches + self.pos_embedding  # positional embedding\n        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n        features = self.layer_norm(self.transformer(patches))\n        logits = self.head(features[:, 0])  # cls tokenのみを入力する\n        return logits\n\n    def get_last_selfattention(self, x):\n        patches = self.patchify(x)\n        patches = patches + self.pos_embedding\n\n        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n        for i, block in enumerate(self.transformer):\n            if i < len(self.transformer) - 1:\n                patches = block(patches)\n            else:\n                return block(patches, return_attn=True)\n\n\nmodel = Classifier(encoder).to(device)\n# DataParallelを使用して複数GPUで並列処理\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\n\n# モデルをデバイスに送る\nmodel = model.to(device)\nepochs = 20\nlr = 0.0001\nwarmup_length = 10\nbatch_size = 512\noptimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\nscheduler = CosineScheduler(epochs, lr, warmup_length)\ncriterion = nn.CrossEntropyLoss()\nstep_count = 0","metadata":{"id":"z8n5wVT-xvv1","execution":{"iopub.status.busy":"2024-09-18T19:36:37.564109Z","iopub.execute_input":"2024-09-18T19:36:37.564945Z","iopub.status.idle":"2024-09-18T19:36:37.596243Z","shell.execute_reply.started":"2024-09-18T19:36:37.564901Z","shell.execute_reply":"2024-09-18T19:36:37.595272Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"}]},{"cell_type":"code","source":"#del model, train_loss, valid_loss\ntorch.cuda.empty_cache()\n# ハイパーパラメータの設定\nconfig = {\n    \"image_size\": [32, 32],\n    \"patch_size\": [2, 2],\n    \"emb_dim\": 192,\n    \"enc_layers\": 12,\n    \"enc_heads\": 3,\n    \"enc_dim_head\": 64,\n    \"enc_mlp_dim\": 192,\n    \"dec_layers\": 4,\n    \"dec_heads\": 3,\n    \"dec_dim_head\": 64,\n    \"dec_mlp_dim\": 192,\n    \"mask_ratio\": 0.75,\n    \"dropout\": 0.\n}\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npretrained_model = MAE_ViT(**config).to(device)\npretrained_model.load_state_dict(new_state_dict)\n\nencoder = pretrained_model.encoder\n\n# モデルの定義\nmodel = Classifier(encoder).to(device)\n\nepochs = 100\nlr = 0.0005\nwarmup_length = 5\nbatch_size = 128\noptimizer = optim.AdamW(model.head.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\nscheduler = CosineScheduler(epochs, lr, warmup_length)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T19:36:37.597368Z","iopub.execute_input":"2024-09-18T19:36:37.597714Z","iopub.status.idle":"2024-09-18T19:36:37.814037Z","shell.execute_reply.started":"2024-09-18T19:36:37.597673Z","shell.execute_reply":"2024-09-18T19:36:37.813292Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"## 分類器の学習（並列処理）","metadata":{}},{"cell_type":"code","source":"# DataParallelを使用して複数GPUで並列処理\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\n\n# モデルをデバイスに送る(必要！)\nmodel = model.to(device)\nfor epoch in range(epochs):\n    new_lr = scheduler(epoch)\n    set_lr(new_lr, optimizer)\n\n    total_train_loss = 0.\n    total_train_acc = 0.\n    total_valid_loss = 0.\n    total_valid_acc = 0.\n    for x, t in dataloader_train:\n        step_count += 1\n        x, t = x.to(device), t.to(device)\n        pred = model(x)\n\n        train_loss = criterion(pred, t)\n        train_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n\n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n\n        total_train_loss += train_loss.item()\n        total_train_acc += train_acc\n\n    with torch.no_grad():\n        for x, t in dataloader_valid:\n            x, t = x.to(device), t.to(device)\n            pred = model(x)\n\n            valid_loss = criterion(pred, t)\n            valid_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n\n            total_valid_loss += valid_loss.item()\n            total_valid_acc += valid_acc\n\n    print(f\"Epoch[{epoch+1} / {epochs}]\",\n          f\"Train Loss: {total_train_loss/len(dataloader_train):.4f}\",\n          f\"Train Acc.: {total_train_acc/len(dataloader_train):.4f}\",\n          f\"Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\",\n          f\"Valid Acc.: {total_valid_acc/len(dataloader_valid):.4f}\")\n","metadata":{"id":"p5T6QiHwyTRj","execution":{"iopub.status.busy":"2024-09-18T19:36:37.817047Z","iopub.execute_input":"2024-09-18T19:36:37.817420Z","iopub.status.idle":"2024-09-18T22:14:00.390232Z","shell.execute_reply.started":"2024-09-18T19:36:37.817388Z","shell.execute_reply":"2024-09-18T22:14:00.389224Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Using 2 GPUs!\nEpoch[1 / 100] Train Loss: 2.2815 Train Acc.: 0.1877 Valid Loss: 2.2578 Valid Acc.: 0.2890\nEpoch[2 / 100] Train Loss: 2.2151 Train Acc.: 0.3347 Valid Loss: 2.1740 Valid Acc.: 0.4121\nEpoch[3 / 100] Train Loss: 2.1186 Train Acc.: 0.4073 Valid Loss: 2.0702 Valid Acc.: 0.3841\nEpoch[4 / 100] Train Loss: 2.0066 Train Acc.: 0.4365 Valid Loss: 1.9527 Valid Acc.: 0.4535\nEpoch[5 / 100] Train Loss: 1.8931 Train Acc.: 0.4643 Valid Loss: 1.8454 Valid Acc.: 0.4748\nEpoch[6 / 100] Train Loss: 1.7928 Train Acc.: 0.4875 Valid Loss: 1.7629 Valid Acc.: 0.4811\nEpoch[7 / 100] Train Loss: 1.7138 Train Acc.: 0.5014 Valid Loss: 1.6886 Valid Acc.: 0.5099\nEpoch[8 / 100] Train Loss: 1.6491 Train Acc.: 0.5164 Valid Loss: 1.6280 Valid Acc.: 0.5202\nEpoch[9 / 100] Train Loss: 1.5960 Train Acc.: 0.5243 Valid Loss: 1.5795 Valid Acc.: 0.5285\nEpoch[10 / 100] Train Loss: 1.5496 Train Acc.: 0.5363 Valid Loss: 1.5373 Valid Acc.: 0.5395\nEpoch[11 / 100] Train Loss: 1.5106 Train Acc.: 0.5451 Valid Loss: 1.5062 Valid Acc.: 0.5462\nEpoch[12 / 100] Train Loss: 1.4762 Train Acc.: 0.5528 Valid Loss: 1.4719 Valid Acc.: 0.5475\nEpoch[13 / 100] Train Loss: 1.4466 Train Acc.: 0.5579 Valid Loss: 1.4445 Valid Acc.: 0.5608\nEpoch[14 / 100] Train Loss: 1.4199 Train Acc.: 0.5639 Valid Loss: 1.4181 Valid Acc.: 0.5602\nEpoch[15 / 100] Train Loss: 1.3969 Train Acc.: 0.5703 Valid Loss: 1.3976 Valid Acc.: 0.5678\nEpoch[16 / 100] Train Loss: 1.3756 Train Acc.: 0.5741 Valid Loss: 1.3727 Valid Acc.: 0.5749\nEpoch[17 / 100] Train Loss: 1.3564 Train Acc.: 0.5783 Valid Loss: 1.3575 Valid Acc.: 0.5831\nEpoch[18 / 100] Train Loss: 1.3390 Train Acc.: 0.5831 Valid Loss: 1.3464 Valid Acc.: 0.5778\nEpoch[19 / 100] Train Loss: 1.3237 Train Acc.: 0.5847 Valid Loss: 1.3251 Valid Acc.: 0.5792\nEpoch[20 / 100] Train Loss: 1.3092 Train Acc.: 0.5884 Valid Loss: 1.3109 Valid Acc.: 0.5889\nEpoch[21 / 100] Train Loss: 1.2966 Train Acc.: 0.5902 Valid Loss: 1.3041 Valid Acc.: 0.5854\nEpoch[22 / 100] Train Loss: 1.2852 Train Acc.: 0.5930 Valid Loss: 1.2943 Valid Acc.: 0.5849\nEpoch[23 / 100] Train Loss: 1.2736 Train Acc.: 0.5958 Valid Loss: 1.2807 Valid Acc.: 0.5847\nEpoch[24 / 100] Train Loss: 1.2635 Train Acc.: 0.5984 Valid Loss: 1.2728 Valid Acc.: 0.5922\nEpoch[25 / 100] Train Loss: 1.2543 Train Acc.: 0.6002 Valid Loss: 1.2649 Valid Acc.: 0.6000\nEpoch[26 / 100] Train Loss: 1.2457 Train Acc.: 0.6016 Valid Loss: 1.2536 Valid Acc.: 0.5911\nEpoch[27 / 100] Train Loss: 1.2376 Train Acc.: 0.6042 Valid Loss: 1.2459 Valid Acc.: 0.5947\nEpoch[28 / 100] Train Loss: 1.2297 Train Acc.: 0.6062 Valid Loss: 1.2368 Valid Acc.: 0.6042\nEpoch[29 / 100] Train Loss: 1.2229 Train Acc.: 0.6074 Valid Loss: 1.2287 Valid Acc.: 0.6045\nEpoch[30 / 100] Train Loss: 1.2168 Train Acc.: 0.6077 Valid Loss: 1.2290 Valid Acc.: 0.6005\nEpoch[31 / 100] Train Loss: 1.2109 Train Acc.: 0.6090 Valid Loss: 1.2192 Valid Acc.: 0.6089\nEpoch[32 / 100] Train Loss: 1.2045 Train Acc.: 0.6115 Valid Loss: 1.2128 Valid Acc.: 0.6079\nEpoch[33 / 100] Train Loss: 1.1999 Train Acc.: 0.6113 Valid Loss: 1.2053 Valid Acc.: 0.6054\nEpoch[34 / 100] Train Loss: 1.1953 Train Acc.: 0.6120 Valid Loss: 1.1996 Valid Acc.: 0.6088\nEpoch[35 / 100] Train Loss: 1.1900 Train Acc.: 0.6143 Valid Loss: 1.2075 Valid Acc.: 0.6086\nEpoch[36 / 100] Train Loss: 1.1857 Train Acc.: 0.6167 Valid Loss: 1.1903 Valid Acc.: 0.6160\nEpoch[37 / 100] Train Loss: 1.1812 Train Acc.: 0.6163 Valid Loss: 1.1903 Valid Acc.: 0.6114\nEpoch[38 / 100] Train Loss: 1.1773 Train Acc.: 0.6173 Valid Loss: 1.1839 Valid Acc.: 0.6136\nEpoch[39 / 100] Train Loss: 1.1742 Train Acc.: 0.6186 Valid Loss: 1.1838 Valid Acc.: 0.6139\nEpoch[40 / 100] Train Loss: 1.1700 Train Acc.: 0.6201 Valid Loss: 1.1752 Valid Acc.: 0.6225\nEpoch[41 / 100] Train Loss: 1.1668 Train Acc.: 0.6208 Valid Loss: 1.1793 Valid Acc.: 0.6102\nEpoch[42 / 100] Train Loss: 1.1633 Train Acc.: 0.6210 Valid Loss: 1.1758 Valid Acc.: 0.6195\nEpoch[43 / 100] Train Loss: 1.1610 Train Acc.: 0.6217 Valid Loss: 1.1676 Valid Acc.: 0.6165\nEpoch[44 / 100] Train Loss: 1.1578 Train Acc.: 0.6225 Valid Loss: 1.1660 Valid Acc.: 0.6208\nEpoch[45 / 100] Train Loss: 1.1548 Train Acc.: 0.6229 Valid Loss: 1.1667 Valid Acc.: 0.6190\nEpoch[46 / 100] Train Loss: 1.1529 Train Acc.: 0.6238 Valid Loss: 1.1651 Valid Acc.: 0.6184\nEpoch[47 / 100] Train Loss: 1.1500 Train Acc.: 0.6244 Valid Loss: 1.1585 Valid Acc.: 0.6221\nEpoch[48 / 100] Train Loss: 1.1478 Train Acc.: 0.6261 Valid Loss: 1.1590 Valid Acc.: 0.6223\nEpoch[49 / 100] Train Loss: 1.1456 Train Acc.: 0.6262 Valid Loss: 1.1523 Valid Acc.: 0.6171\nEpoch[50 / 100] Train Loss: 1.1448 Train Acc.: 0.6252 Valid Loss: 1.1577 Valid Acc.: 0.6149\nEpoch[51 / 100] Train Loss: 1.1419 Train Acc.: 0.6263 Valid Loss: 1.1521 Valid Acc.: 0.6252\nEpoch[52 / 100] Train Loss: 1.1399 Train Acc.: 0.6268 Valid Loss: 1.1478 Valid Acc.: 0.6222\nEpoch[53 / 100] Train Loss: 1.1383 Train Acc.: 0.6284 Valid Loss: 1.1508 Valid Acc.: 0.6232\nEpoch[54 / 100] Train Loss: 1.1364 Train Acc.: 0.6273 Valid Loss: 1.1452 Valid Acc.: 0.6249\nEpoch[55 / 100] Train Loss: 1.1351 Train Acc.: 0.6287 Valid Loss: 1.1406 Valid Acc.: 0.6254\nEpoch[56 / 100] Train Loss: 1.1338 Train Acc.: 0.6283 Valid Loss: 1.1470 Valid Acc.: 0.6268\nEpoch[57 / 100] Train Loss: 1.1324 Train Acc.: 0.6291 Valid Loss: 1.1469 Valid Acc.: 0.6171\nEpoch[58 / 100] Train Loss: 1.1307 Train Acc.: 0.6291 Valid Loss: 1.1392 Valid Acc.: 0.6280\nEpoch[59 / 100] Train Loss: 1.1291 Train Acc.: 0.6298 Valid Loss: 1.1415 Valid Acc.: 0.6228\nEpoch[60 / 100] Train Loss: 1.1283 Train Acc.: 0.6298 Valid Loss: 1.1394 Valid Acc.: 0.6243\nEpoch[61 / 100] Train Loss: 1.1268 Train Acc.: 0.6305 Valid Loss: 1.1389 Valid Acc.: 0.6243\nEpoch[62 / 100] Train Loss: 1.1263 Train Acc.: 0.6304 Valid Loss: 1.1342 Valid Acc.: 0.6319\nEpoch[63 / 100] Train Loss: 1.1253 Train Acc.: 0.6309 Valid Loss: 1.1353 Valid Acc.: 0.6287\nEpoch[64 / 100] Train Loss: 1.1240 Train Acc.: 0.6314 Valid Loss: 1.1300 Valid Acc.: 0.6270\nEpoch[65 / 100] Train Loss: 1.1237 Train Acc.: 0.6311 Valid Loss: 1.1358 Valid Acc.: 0.6209\nEpoch[66 / 100] Train Loss: 1.1218 Train Acc.: 0.6320 Valid Loss: 1.1322 Valid Acc.: 0.6277\nEpoch[67 / 100] Train Loss: 1.1211 Train Acc.: 0.6317 Valid Loss: 1.1351 Valid Acc.: 0.6283\nEpoch[68 / 100] Train Loss: 1.1204 Train Acc.: 0.6322 Valid Loss: 1.1313 Valid Acc.: 0.6271\nEpoch[69 / 100] Train Loss: 1.1208 Train Acc.: 0.6312 Valid Loss: 1.1305 Valid Acc.: 0.6315\nEpoch[70 / 100] Train Loss: 1.1200 Train Acc.: 0.6323 Valid Loss: 1.1286 Valid Acc.: 0.6259\nEpoch[71 / 100] Train Loss: 1.1183 Train Acc.: 0.6325 Valid Loss: 1.1273 Valid Acc.: 0.6288\nEpoch[72 / 100] Train Loss: 1.1179 Train Acc.: 0.6336 Valid Loss: 1.1255 Valid Acc.: 0.6323\nEpoch[73 / 100] Train Loss: 1.1175 Train Acc.: 0.6326 Valid Loss: 1.1254 Valid Acc.: 0.6292\nEpoch[74 / 100] Train Loss: 1.1173 Train Acc.: 0.6324 Valid Loss: 1.1307 Valid Acc.: 0.6300\nEpoch[75 / 100] Train Loss: 1.1172 Train Acc.: 0.6326 Valid Loss: 1.1266 Valid Acc.: 0.6316\nEpoch[76 / 100] Train Loss: 1.1170 Train Acc.: 0.6339 Valid Loss: 1.1272 Valid Acc.: 0.6300\nEpoch[77 / 100] Train Loss: 1.1165 Train Acc.: 0.6327 Valid Loss: 1.1293 Valid Acc.: 0.6254\nEpoch[78 / 100] Train Loss: 1.1152 Train Acc.: 0.6348 Valid Loss: 1.1216 Valid Acc.: 0.6318\nEpoch[79 / 100] Train Loss: 1.1152 Train Acc.: 0.6342 Valid Loss: 1.1292 Valid Acc.: 0.6286\nEpoch[80 / 100] Train Loss: 1.1151 Train Acc.: 0.6337 Valid Loss: 1.1248 Valid Acc.: 0.6312\nEpoch[81 / 100] Train Loss: 1.1144 Train Acc.: 0.6332 Valid Loss: 1.1195 Valid Acc.: 0.6324\nEpoch[82 / 100] Train Loss: 1.1141 Train Acc.: 0.6340 Valid Loss: 1.1266 Valid Acc.: 0.6292\nEpoch[83 / 100] Train Loss: 1.1145 Train Acc.: 0.6338 Valid Loss: 1.1218 Valid Acc.: 0.6330\nEpoch[84 / 100] Train Loss: 1.1134 Train Acc.: 0.6345 Valid Loss: 1.1243 Valid Acc.: 0.6323\nEpoch[85 / 100] Train Loss: 1.1137 Train Acc.: 0.6339 Valid Loss: 1.1242 Valid Acc.: 0.6323\nEpoch[86 / 100] Train Loss: 1.1128 Train Acc.: 0.6345 Valid Loss: 1.1270 Valid Acc.: 0.6288\nEpoch[87 / 100] Train Loss: 1.1132 Train Acc.: 0.6342 Valid Loss: 1.1240 Valid Acc.: 0.6319\nEpoch[88 / 100] Train Loss: 1.1129 Train Acc.: 0.6342 Valid Loss: 1.1230 Valid Acc.: 0.6290\nEpoch[89 / 100] Train Loss: 1.1130 Train Acc.: 0.6335 Valid Loss: 1.1254 Valid Acc.: 0.6292\nEpoch[90 / 100] Train Loss: 1.1134 Train Acc.: 0.6339 Valid Loss: 1.1205 Valid Acc.: 0.6320\nEpoch[91 / 100] Train Loss: 1.1131 Train Acc.: 0.6340 Valid Loss: 1.1256 Valid Acc.: 0.6283\nEpoch[92 / 100] Train Loss: 1.1130 Train Acc.: 0.6342 Valid Loss: 1.1172 Valid Acc.: 0.6334\nEpoch[93 / 100] Train Loss: 1.1132 Train Acc.: 0.6340 Valid Loss: 1.1280 Valid Acc.: 0.6265\nEpoch[94 / 100] Train Loss: 1.1128 Train Acc.: 0.6342 Valid Loss: 1.1239 Valid Acc.: 0.6306\nEpoch[95 / 100] Train Loss: 1.1127 Train Acc.: 0.6341 Valid Loss: 1.1255 Valid Acc.: 0.6284\nEpoch[96 / 100] Train Loss: 1.1130 Train Acc.: 0.6338 Valid Loss: 1.1193 Valid Acc.: 0.6318\nEpoch[97 / 100] Train Loss: 1.1127 Train Acc.: 0.6343 Valid Loss: 1.1200 Valid Acc.: 0.6330\nEpoch[98 / 100] Train Loss: 1.1126 Train Acc.: 0.6342 Valid Loss: 1.1192 Valid Acc.: 0.6327\nEpoch[99 / 100] Train Loss: 1.1122 Train Acc.: 0.6345 Valid Loss: 1.1211 Valid Acc.: 0.6305\nEpoch[100 / 100] Train Loss: 1.1125 Train Acc.: 0.6342 Valid Loss: 1.1208 Valid Acc.: 0.6314\n","output_type":"stream"}]},{"cell_type":"code","source":"model_Vit_path=\"/kaggle/working/ViT_supervised_params.pth\"\ntorch.save(model.state_dict(), model_Vit_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T22:14:00.391581Z","iopub.execute_input":"2024-09-18T22:14:00.392153Z","iopub.status.idle":"2024-09-18T22:14:00.441179Z","shell.execute_reply.started":"2024-09-18T22:14:00.392107Z","shell.execute_reply":"2024-09-18T22:14:00.440254Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nt_pred = []\nfor x in dataloader_test:\n    x = x.to(device)\n    y = model(x)\n\n    # モデルの出力を予測値のスカラーに変換\n    pred = y.argmax(1).tolist()\n    t_pred.extend(pred)\n\nsubmission = pd.Series(t_pred, name='label')\nsubmission_path=\"/kaggle/working/submission_pred.csv\"\nsubmission.to_csv(submission_path, header=True, index_label='id')","metadata":{"id":"72n19Q_RyqWl","execution":{"iopub.status.busy":"2024-09-18T22:14:00.442386Z","iopub.execute_input":"2024-09-18T22:14:00.442688Z","iopub.status.idle":"2024-09-18T22:14:08.637114Z","shell.execute_reply.started":"2024-09-18T22:14:00.442655Z","shell.execute_reply":"2024-09-18T22:14:08.636319Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"NwKJ8IBcy5TG"},"execution_count":null,"outputs":[]}]}