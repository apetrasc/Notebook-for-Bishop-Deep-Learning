{"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9413607,"sourceType":"datasetVersion","datasetId":5716814}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 概要\n自己教師あり学習を用いて事前学習を行い，得られた表現をLinear probingで評価してみたノート。 Kaggle のT4を2台使うために適宜並列化してあります。 注意事項として、並列化に伴い、モデルを保存・ロードする際にやや工夫が必要になることがあります。特に、学習の直前で\n```\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel = model.to(device)\n```\nを実行すること、そしてモデルを読み込む際は\n```\nimport matplotlib as plt\nstate_dict = torch.load(model_path, map_location=device, weights_only=True)\nfrom collections import OrderedDict\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k.replace('module.', '')  # module.のプレフィックスを削除\n    new_state_dict[name] = v\n\nmodel.load_state_dict(new_state_dict)\n```\nによって'module.'を削除しなければならないということが挙げられます。\nTransformerは並列計算に特化した手法である（Bishop et al, 2023）ため、並列計算のメソッドを書き加えておくことは重要です。Kaggle無料版の計算資源をフル活用したCidar10用Tutorialコードとなっています。\nEpochは、20～30程度では暖機運転の途中となりほとんど意味を成しません。Epochは100以上を強く推奨します。元のノートブックでは事前学習にEpoch2000、分類器の学習にEpoch100回を使用していました。\n\n","metadata":{"id":"EUU-McVcFGJ0"}},{"cell_type":"markdown","source":"## 必要パッケージのインストール","metadata":{}},{"cell_type":"code","source":"!pip install einops","metadata":{"id":"rteXenmbw49z","outputId":"a2d5a57c-668f-4748-cb5e-8ad13692daf3","execution":{"iopub.status.busy":"2024-09-18T15:16:48.918842Z","iopub.execute_input":"2024-09-18T15:16:48.919591Z","iopub.status.idle":"2024-09-18T15:17:01.627734Z","shell.execute_reply.started":"2024-09-18T15:16:48.919547Z","shell.execute_reply":"2024-09-18T15:17:01.626738Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.8.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torchvision import transforms\nfrom tqdm import tqdm_notebook as tqdm\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch.optim as optim\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\nimport math\n#学習データ,テストデータのpath\nx_train_path='/kaggle/input/cifar10/data/x_train.npy'\nt_train_path='/kaggle/input/cifar10/data/t_train.npy'\nx_test_path='/kaggle/input/cifar10/data/x_test.npy'\n\nx_train = np.load(x_train_path)\nt_train = np.load(t_train_path)\nx_test = np.load(x_test_path)\n\nclass train_dataset(torch.utils.data.Dataset):\n    def __init__(self, x_train, t_train):\n        data = x_train.astype('float32')\n        self.x_train = []\n        for i in range(data.shape[0]):\n            self.x_train.append(Image.fromarray(np.uint8(data[i])))\n        self.t_train = t_train\n        self.transform = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.x_train)\n\n    def __getitem__(self, idx):\n        return self.transform(self.x_train[idx]), torch.tensor(t_train[idx], dtype=torch.long)\n\nclass test_dataset(torch.utils.data.Dataset):\n    def __init__(self, x_test):\n        data = x_test.astype('float32')\n        self.x_test = []\n        for i in range(data.shape[0]):\n            self.x_test.append(Image.fromarray(np.uint8(data[i])))\n        self.transform = transforms.ToTensor()\n\n    def __len__(self):\n        return len(self.x_test)\n\n    def __getitem__(self, idx):\n        return self.transform(self.x_test[idx])\n\ntrainval_data = train_dataset(x_train, t_train)\ntest_data = test_dataset(x_test)","metadata":{"id":"LNpUF5xOJ8bG","execution":{"iopub.status.busy":"2024-09-18T15:17:01.629828Z","iopub.execute_input":"2024-09-18T15:17:01.630170Z","iopub.status.idle":"2024-09-18T15:17:03.477409Z","shell.execute_reply.started":"2024-09-18T15:17:01.630135Z","shell.execute_reply":"2024-09-18T15:17:03.476608Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## データローダの準備  ","metadata":{"id":"fSqA6Ni3MDSX"}},{"cell_type":"code","source":"val_size = 3000\ntrain_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n\ntrain_transform = transforms.Compose(\n    [transforms.RandomCrop(32, padding=4),\n     transforms.RandomHorizontalFlip(),\n     transforms.ToTensor(),\n     transforms.Normalize(0.5, 0.5)]\n)\nvalid_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize(0.5, 0.5)]\n)\n\nbatch_size = 128\n\ndataloader_train = torch.utils.data.DataLoader(\n    train_data,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndataloader_valid = torch.utils.data.DataLoader(\n    valid_data,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndataloader_test = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=batch_size,\n    shuffle=False\n)","metadata":{"id":"63ODMwChMEy_","execution":{"iopub.status.busy":"2024-09-18T15:17:03.478648Z","iopub.execute_input":"2024-09-18T15:17:03.478959Z","iopub.status.idle":"2024-09-18T15:17:03.645032Z","shell.execute_reply.started":"2024-09-18T15:17:03.478926Z","shell.execute_reply":"2024-09-18T15:17:03.644204Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## その他必要な関数やモデルなど","metadata":{}},{"cell_type":"code","source":"def fix_seed(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n\nfix_seed(seed=42)\n\n\ndef random_indexes(size):\n    \"\"\"\n    パッチをランダムに並べ替えるためのindexを生成する関数．\n\n    Argument\n    --------\n    size : int\n        入力されるパッチの数（系列長Nと同じ値）．\n    \"\"\"\n    forward_indexes = np.arange(size)  # 0からsizeまでを並べた配列を作成\n    np.random.shuffle(forward_indexes)  # 生成した配列をシャッフルすることで，パッチの順番をランダムに決定\n    backward_indexes = np.argsort(forward_indexes)  # 並べ替えたパッチをもとの順番に戻すためのidx\n\n    return forward_indexes, backward_indexes\n\n\ndef take_indexes(sequences, indexes):\n    \"\"\"\n    パッチを並べ替えるための関数．\n\n    Argument\n    --------\n    sequences : torch.Tensor\n        入力画像をパッチ分割したデータ．(B, N, dim)の形状をしている．\n    indexes : np.ndarray\n        並べ替えるために利用するindex．\n        random_indexesで生成したforward_indexesかbackward_indexesが入ることが想定されている．\n    \"\"\"\n    return torch.gather(sequences, dim=1, index=indexes.unsqueeze(2).repeat(1, 1, sequences.shape[-1]))\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads, dim_head, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n        dim : int\n            入力データの次元数．埋め込み次元数と一致する．\n        heads : int\n            ヘッドの数．\n        dim_head : int\n            各ヘッドのデータの次元数．\n        dropout : float\n            Dropoutの確率(default=0.)．\n        \"\"\"\n        super().__init__()\n\n        self.dim = dim\n        self.dim_head = dim_head\n        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n\n        self.heads = heads\n        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n\n        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n        self.dropout = nn.Dropout(dropout)\n\n        # Q, K, Vに変換するための全結合層\n        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n\n        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n        self.to_out = nn.Sequential(\n            nn.Linear(in_features=inner_dim, out_features=dim),\n            nn.Dropout(dropout),\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        \"\"\"\n        B: バッチサイズ\n        N: 系列長\n        D: データの次元数(dim)\n        \"\"\"\n        B, N, D = x.size()\n\n        # 入力データをQ, K, Vに変換する\n        # (B, N, dim) -> (B, N, inner_dim)\n        q = self.to_q(x)\n        k = self.to_k(x)\n        v = self.to_v(x)\n\n        # Q, K, Vをヘッドに分割する\n        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n\n        # QK^T / sqrt(d_k)を計算する\n        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n\n        # ソフトマックス関数でスコアを算出し，Dropoutをする\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        # softmax(QK^T / sqrt(d_k))Vを計算する\n        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n        out = torch.matmul(attn ,v)\n\n        # もとの形に戻す\n        # (B, heads, N, dim_head) -> (B, N, dim)\n        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n\n        # 次元が違っていればもとに戻して出力\n        # 表現の可視化のためにattention mapも返すようにしておく\n        return self.to_out(out), attn\n\n\nclass FFN(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n        dim : int\n            入力データの次元数．\n        hidden_dim : int\n            隠れ層の次元．\n        dropout : float\n            各全結合層の後のDropoutの確率(default=0.)．\n        \"\"\"\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(in_features=dim, out_features=hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(in_features=hidden_dim, out_features=dim),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        \"\"\"\n        (B, D) -> (B, D)\n        B: バッチサイズ\n        D: 次元数\n        \"\"\"\n        return self.net(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n        \"\"\"\n        TransformerのEncoder Blockの実装．\n\n        Arguments\n        ---------\n        dim : int\n            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n        heads : int\n            Multi-Head Attentionのヘッドの数．\n        dim_head : int\n            Multi-Head Attentionの各ヘッドの次元数．\n        mlp_dim : int\n            Feed-Forward Networkの隠れ層の次元数．\n        dropout : float\n            Droptou層の確率p．\n        \"\"\"\n        super().__init__()\n\n        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n        self.attn = Attention(dim, heads, dim_head, dropout)\n        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n        self.ffn = FFN(dim, mlp_dim, dropout)\n\n    def forward(self, x, return_attn=False):\n        \"\"\"\n        x: (B, N, dim)\n        B: バッチサイズ\n        N: 系列長\n        dim: 埋め込み次元\n        \"\"\"\n        y, attn = self.attn(self.attn_ln(x))\n        if return_attn:  # attention mapを返す（attention mapの可視化に利用）\n            return attn\n        x = y + x\n        out = self.ffn(self.ffn_ln(x)) + x\n\n        return out\n\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n        \"\"\"\n        入力画像をパッチごとに埋め込むための層．\n\n        Arguments\n        ---------\n        image_size : Tuple[int]\n            入力画像のサイズ．\n        patch_size : Tuple[int]\n            各パッチのサイズ．\n        in_channels : int\n            入力画像のチャネル数．\n        embed_dim : int\n            埋め込み後の次元数．\n        \"\"\"\n        super().__init__()\n\n        image_height, image_width = image_size\n        patch_height, patch_width = patch_size\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n        )\n\n    def forward(self, x):\n        \"\"\"\n        B: バッチサイズ\n        C: 入力画像のチャネル数\n        H: 入力画像の高さ\n        W: 入力画像の幅\n        \"\"\"\n        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)\n\n\nclass PatchShuffle(nn.Module):\n    def __init__(self, ratio):\n        # ratio: Encoderに入力しないパッチの割合\n        super().__init__()\n        self.ratio = ratio\n\n    def forward(self, patches):\n        \"\"\"\n        B: バッチサイズ\n        N: 系列長（＝パッチの数）\n        dim: 次元数（＝埋め込みの次元数）\n        \"\"\"\n        B, N, dim = patches.shape\n        remain_N = int(N * (1 - self.ratio))  # Encoderに入力するパッチの数\n\n        indexes = [random_indexes(N) for _ in range(B)]  # バッチごとに異なる順番のindexを作る\n        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # バッチを並べ替えるときのidx (B, N)\n        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # 並べ替えたパッチをもとの順番に戻すためのidx  (B, N)\n\n        patches = take_indexes(patches, forward_indexes)  # パッチを並べ替える\n        patches = patches[:, :remain_N, :]  # Encoderに入力するパッチを抽出\n\n        return patches, forward_indexes, backward_indexes\n\n\nclass MAE_Encoder(torch.nn.Module):\n    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=12,\n                 heads=3, dim_head=64, mlp_dim=192, mask_ratio=0.75, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n\n        image_size : List[int]\n            入力画像の大きさ．\n        patch_size : List[int]\n            各パッチの大きさ．\n        emb_dim : int\n            データを埋め込む次元の数．\n        num_layer : int\n            Encoderに含まれるBlockの数．\n        heads : int\n            Multi-Head Attentionのヘッドの数．\n        dim_head : int\n            Multi-Head Attentionの各ヘッドの次元数．\n        mlp_dim : int\n            Feed-Forward Networkの隠れ層の次元数．\n        mask_ratio : float\n            入力パッチのマスクする割合．\n        dropout : float\n            ドロップアウトの確率．\n        \"\"\"\n        super().__init__()\n        img_height, img_width = image_size\n        patch_height, patch_width = patch_size\n        num_patches = (img_height // patch_height) * (img_width // patch_width)\n\n        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, emb_dim))  # class tokenの初期化\n        self.pos_embedding = torch.nn.Parameter(torch.randn(1, num_patches, emb_dim))  # positional embedding（学習可能にしている）\n        self.shuffle = PatchShuffle(mask_ratio)\n\n        # 入力画像をパッチに分割する\n        self.patchify = PatchEmbedding(image_size, patch_size, 3, emb_dim)\n\n        # Encoder（Blockを重ねる）\n        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n\n        self.layer_norm = nn.LayerNorm(emb_dim)\n\n        self.init_weight()\n\n    def init_weight(self):\n        torch.nn.init.normal_(self.cls_token, std=0.02)\n        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n\n    def forward(self, img):\n        # 1. 入力画像をパッチに分割して，positional embeddingする\n        patches = self.patchify(img)\n        patches = patches + self.pos_embedding\n\n        # 2. 分割したパッチをランダムに並べ替えて，必要なパッチのみ得る\n        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n\n        # class tokenを結合\n        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)\n\n        # 3. Encoderで入力データを処理する\n        features = self.layer_norm(self.transformer(patches))\n\n        return features, backward_indexes\n\n\nclass MAE_Decoder(nn.Module):\n    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=4,\n                 heads=3, dim_head=64, mlp_dim=192, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n\n        image_size : List[int]\n            入力画像の大きさ．\n        patch_size : List[int]\n            各パッチの大きさ．\n        emb_dim : int\n            データを埋め込む次元の数．\n        num_layer : int\n            Decoderに含まれるBlockの数．\n        heads : int\n            Multi-Head Attentionのヘッドの数．\n        dim_head : int\n            Multi-Head Attentionの各ヘッドの次元数．\n        mlp_dim : int\n            Feed-Forward Networkの隠れ層の次元数．\n        dropout : float\n            ドロップアウトの確率．\n        \"\"\"\n        super().__init__()\n        img_height, img_width = image_size\n        patch_height, patch_width = patch_size\n        num_patches = (img_height // patch_height) * (img_width // patch_width)\n\n        self.mask_token = torch.nn.Parameter(torch.rand(1, 1, emb_dim))\n        self.pos_embedding = torch.nn.Parameter(torch.rand(1, num_patches+1, emb_dim))\n\n        # Decoder(Blockを重ねる）\n        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n\n        # 埋め込みされた表現から画像を復元するためのhead\n        self.head = torch.nn.Linear(emb_dim, 3 * patch_height * patch_width)\n        # (B, N, dim)から(B, C, H, W)にreshapeするためのインスタンス\n        self.patch2img = Rearrange(\"b (h w) (c p1 p2) -> b c (h p1) (w p2)\", p1=patch_height, p2=patch_width, h=img_height // patch_height)\n\n        self.init_weight()\n\n    def init_weight(self):\n        torch.nn.init.normal_(self.mask_token, std=0.02)\n        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n\n    def forward(self, features, backward_indexes):\n        # 系列長\n        T = features.shape[1]\n\n        # class tokenがある分backward_indexesの最初に0を追加する\n        # .toはデバイスの変更でよく利用するが，tensorを渡すことでdtypeを変えることができる\n        backward_indexes = torch.cat([torch.zeros(backward_indexes.shape[0], 1).to(backward_indexes), backward_indexes+1], dim=1)\n\n        # 1. mask_tokenを結合して並べ替える．\n        # (B, N*(1-mask_ratio)+1, dim) -> (B, N+1, dim)\n        features = torch.cat([features, self.mask_token.repeat(features.shape[0], backward_indexes.shape[1] - features.shape[1], 1)], dim=1)\n        features = take_indexes(features, backward_indexes)\n        features = features + self.pos_embedding\n\n        features = self.transformer(features)\n\n        # class tokenを除去する\n        # (B, N+1, dim) -> (B, N, dim)\n        features = features[:, 1:, :]\n\n        # 2. 画像を再構成する．\n        # (B, N, dim) -> (B, N, 3 * patch_height * patch_width)\n        patches = self.head(features)\n\n        # MAEではマスクした部分でのみ損失関数を計算するため，maskも一緒に返す\n        mask = torch.zeros_like(patches)\n        mask[:, T-1:] = 1  # cls tokenを含めていた分ずらしている\n        mask = take_indexes(mask, backward_indexes[:, 1:] - 1)\n\n        img = self.patch2img(patches)\n        mask = self.patch2img(mask)\n\n        return img, mask\n\n\nclass MAE_ViT(torch.nn.Module):\n    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192,\n                 enc_layers=12, enc_heads=3, enc_dim_head=64, enc_mlp_dim=768,\n                 dec_layers=4, dec_heads=3, dec_dim_head=64, dec_mlp_dim=768,\n                 mask_ratio=0.75, dropout=0.):\n        \"\"\"\n        Arguments\n        ---------\n        image_size : List[int]\n            入力画像の大きさ．\n        patch_size : List[int]\n            各パッチの大きさ．\n        emb_dim : int\n            データを埋め込む次元の数．\n        {enc/dec}_layers : int\n            Encoder / Decoderに含まれるBlockの数．\n        {enc/dec}_heads : int\n            Encoder / DecoderのMulti-Head Attentionのヘッドの数．\n        {enc/dec}_dim_head : int\n            Encoder / DecoderのMulti-Head Attentionの各ヘッドの次元数．\n        {enc/dec}_mlp_dim : int\n            Encoder / DecoderのFeed-Forward Networkの隠れ層の次元数．\n        mask_ratio : float\n            入力パッチのマスクする割合．\n        dropout : float\n            ドロップアウトの確率．\n        \"\"\"\n        super().__init__()\n\n        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, enc_layers,\n                                   enc_heads, enc_dim_head, enc_mlp_dim, mask_ratio, dropout)\n        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, dec_layers,\n                                   dec_heads, dec_dim_head, dec_mlp_dim, dropout)\n\n    def forward(self, img):\n        features, backward_indexes = self.encoder(img)\n        rec_img, mask = self.decoder(features, backward_indexes)\n        return rec_img, mask\n\n    def get_last_selfattention(self, x):\n        patches = self.encoder.patchify(x)\n        patches = patches + self.encoder.pos_embedding\n\n        patches = torch.cat([self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n        for i, block in enumerate(self.encoder.transformer):\n            if i < len(self.encoder.transformer) - 1:\n                patches = block(patches)\n            else:\n                return block(patches, return_attn=True)\n\nconfig = {\n    \"image_size\": [32, 32],\n    \"patch_size\": [2, 2],\n    \"emb_dim\": 192,\n    \"enc_layers\": 12,\n    \"enc_heads\": 3,\n    \"enc_dim_head\": 64,\n    \"enc_mlp_dim\": 192,\n    \"dec_layers\": 4,\n    \"dec_heads\": 3,\n    \"dec_dim_head\": 64,\n    \"dec_mlp_dim\": 192,\n    \"mask_ratio\": 0.75,\n    \"dropout\": 0.\n}\n# cosine scheduler\nclass CosineScheduler:\n    def __init__(self, epochs, lr, warmup_length=5):\n        \"\"\"\n        Arguments\n        ---------\n        epochs : int\n            学習のエポック数．\n        lr : float\n            学習率．\n        warmup_length : int\n            warmupを適用するエポック数．\n        \"\"\"\n        self.epochs = epochs\n        self.lr = lr\n        self.warmup = warmup_length\n\n    def __call__(self, epoch):\n        \"\"\"\n        Arguments\n        ---------\n        epoch : int\n            現在のエポック数．\n        \"\"\"\n        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n        progress = np.clip(progress, 0.0, 1.0)\n        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n\n        if self.warmup:\n            lr = lr * min(1., (epoch+1) / self.warmup)\n\n        return lr\ndef set_lr(lr, optimizer):\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = MAE_ViT(**config).to(device)\nepochs = 10\nlr = 0.0024\nbatch_size = 512\nstep_count = 0\nwarmup_length = 20\noptimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.05)\nscheduler = CosineScheduler(epochs, lr, warmup_length)","metadata":{"id":"TzlJ4q1uKagF","execution":{"iopub.status.busy":"2024-09-18T15:17:03.647549Z","iopub.execute_input":"2024-09-18T15:17:03.647865Z","iopub.status.idle":"2024-09-18T15:17:03.776252Z","shell.execute_reply.started":"2024-09-18T15:17:03.647832Z","shell.execute_reply":"2024-09-18T15:17:03.775518Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## 事前学習（自己教師あり学習）の実行","metadata":{"id":"uR8uNlkCxo3d"}},{"cell_type":"code","source":"# DataParallelを使用して複数GPUで並列処理\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\n\n# モデルをデバイスに送る\nmodel = model.to(device)\n\n# 訓練ループ\nfor epoch in range(epochs):\n    # 学習率の更新\n    new_lr = scheduler(epoch)\n    set_lr(new_lr, optimizer)\n\n    total_train_loss = 0.\n    total_valid_loss = 0.\n\n    # モデルの訓練\n    for x, _ in dataloader_train:\n        step_count += 1\n        model.train()\n\n        x = x.to(device)\n        rec_img, mask = model(x)  # ここで並列処理\n\n        train_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n        train_loss.backward()\n\n        if step_count % 8 == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        total_train_loss += train_loss.item()\n\n    # モデルの評価\n    with torch.no_grad():\n        for x, _ in dataloader_valid:\n            model.eval()\n            x = x.to(device)\n            rec_img, mask = model(x)\n            valid_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n            total_valid_loss += valid_loss.item()\n\n    print(f\"Epoch[{epoch+1} / {epochs}] Train Loss: {total_train_loss/len(dataloader_train):.4f} Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\")\n\n# モデルの保存\nmodel_path=\"/kaggle/working/MAE_pretrain_params.pth\"\ntorch.save(model.state_dict(), model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T15:17:03.777368Z","iopub.execute_input":"2024-09-18T15:17:03.777672Z","iopub.status.idle":"2024-09-18T15:27:50.524191Z","shell.execute_reply.started":"2024-09-18T15:17:03.777640Z","shell.execute_reply":"2024-09-18T15:27:50.523032Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Using 2 GPUs!\nEpoch[1 / 10] Train Loss: 0.3228 Valid Loss: 0.3264\nEpoch[2 / 10] Train Loss: 0.3228 Valid Loss: 0.3263\nEpoch[3 / 10] Train Loss: 0.3228 Valid Loss: 0.3267\nEpoch[4 / 10] Train Loss: 0.3226 Valid Loss: 0.3265\nEpoch[5 / 10] Train Loss: 0.3227 Valid Loss: 0.3270\nEpoch[6 / 10] Train Loss: 0.3227 Valid Loss: 0.3269\nEpoch[7 / 10] Train Loss: 0.3227 Valid Loss: 0.3264\nEpoch[8 / 10] Train Loss: 0.3227 Valid Loss: 0.3263\nEpoch[9 / 10] Train Loss: 0.3228 Valid Loss: 0.3270\nEpoch[10 / 10] Train Loss: 0.3228 Valid Loss: 0.3268\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# モデルの定義\nmodel = MAE_ViT(**config).to(device)\n\n# 保存したstate_dictをロード\nstate_dict = torch.load(model_path, map_location=device, weights_only=True)\nfrom collections import OrderedDict\n\n# DataParallelの'module.'プレフィックスを削除\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k.replace('module.', '')  # 'module.'のプレフィックスを削除\n    new_state_dict[name] = v\n\n# ロードしたパラメータをモデルにセット\nmodel.load_state_dict(new_state_dict)\n\n# 評価モードに設定\nmodel.eval()\n\n# データを取得して推論を行う\nx, _ = next(iter(dataloader_valid))\nwith torch.no_grad():\n    rec_img, mask = model(x.to(device))\n\n# CPUに移して結果を確認\nx, rec_img, mask = x.to(\"cpu\"), rec_img.to(\"cpu\"), mask.to(\"cpu\")\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-18T15:30:54.165701Z","iopub.execute_input":"2024-09-18T15:30:54.166422Z","iopub.status.idle":"2024-09-18T15:30:54.462963Z","shell.execute_reply.started":"2024-09-18T15:30:54.166382Z","shell.execute_reply":"2024-09-18T15:30:54.461936Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Linear probing","metadata":{"id":"hHOBi4auxuPR"}},{"cell_type":"code","source":"val_size = 3000\ntrain_data, valid_data = torch.utils.data.random_split(trainval_data, [len(trainval_data) - val_size, val_size])\n\ntrain_transform = transforms.Compose(\n    [transforms.RandomCrop(32, padding=4),\n     transforms.RandomHorizontalFlip(),\n     transforms.ToTensor(),\n     transforms.Normalize(0.5, 0.5)]\n)\nvalid_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize(0.5, 0.5)]\n)\n\nbatch_size = 128\n\ndataloader_train = torch.utils.data.DataLoader(\n    train_data,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndataloader_valid = torch.utils.data.DataLoader(\n    valid_data,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndataloader_test = torch.utils.data.DataLoader(\n    test_data,\n    batch_size=batch_size,\n    shuffle=False\n)","metadata":{"id":"1GeuhPBryfQa","execution":{"iopub.status.busy":"2024-09-18T15:30:58.686691Z","iopub.execute_input":"2024-09-18T15:30:58.687104Z","iopub.status.idle":"2024-09-18T15:30:58.698665Z","shell.execute_reply.started":"2024-09-18T15:30:58.687064Z","shell.execute_reply":"2024-09-18T15:30:58.697762Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, encoder: MAE_Encoder, num_classes=10):\n        super().__init__()\n        self.cls_token = encoder.cls_token\n        self.pos_embedding = encoder.pos_embedding\n        self.patchify = encoder.patchify\n        self.transformer = encoder.transformer\n        self.layer_norm = encoder.layer_norm\n        self.head = nn.Linear(self.pos_embedding.shape[-1], num_classes)\n\n    def forward(self, img):\n        patches = self.patchify(img)\n        patches = patches + self.pos_embedding  # positional embedding\n        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n        features = self.layer_norm(self.transformer(patches))\n        logits = self.head(features[:, 0])  # cls tokenのみを入力する\n        return logits\n\n    def get_last_selfattention(self, x):\n        patches = self.patchify(x)\n        patches = patches + self.pos_embedding\n\n        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n        for i, block in enumerate(self.transformer):\n            if i < len(self.transformer) - 1:\n                patches = block(patches)\n            else:\n                return block(patches, return_attn=True)\n\n\nmodel = Classifier(encoder).to(device)\n# DataParallelを使用して複数GPUで並列処理\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\n\n# モデルをデバイスに送る\nmodel = model.to(device)\nepochs = 20\nlr = 0.0001\nwarmup_length = 10\nbatch_size = 512\noptimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\nscheduler = CosineScheduler(epochs, lr, warmup_length)\ncriterion = nn.CrossEntropyLoss()\nstep_count = 0","metadata":{"id":"z8n5wVT-xvv1","execution":{"iopub.status.busy":"2024-09-18T15:31:01.073709Z","iopub.execute_input":"2024-09-18T15:31:01.074569Z","iopub.status.idle":"2024-09-18T15:31:01.104258Z","shell.execute_reply.started":"2024-09-18T15:31:01.074525Z","shell.execute_reply":"2024-09-18T15:31:01.103388Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"}]},{"cell_type":"code","source":"#del model, train_loss, valid_loss\ntorch.cuda.empty_cache()\n# ハイパーパラメータの設定\nconfig = {\n    \"image_size\": [32, 32],\n    \"patch_size\": [2, 2],\n    \"emb_dim\": 192,\n    \"enc_layers\": 12,\n    \"enc_heads\": 3,\n    \"enc_dim_head\": 64,\n    \"enc_mlp_dim\": 192,\n    \"dec_layers\": 4,\n    \"dec_heads\": 3,\n    \"dec_dim_head\": 64,\n    \"dec_mlp_dim\": 192,\n    \"mask_ratio\": 0.75,\n    \"dropout\": 0.\n}\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npretrained_model = MAE_ViT(**config).to(device)\npretrained_model.load_state_dict(new_state_dict)\n\nencoder = pretrained_model.encoder\n\n# モデルの定義\nmodel = Classifier(encoder).to(device)\n\nepochs = 10\nlr = 0.0005\nwarmup_length = 5\nbatch_size = 128\noptimizer = optim.AdamW(model.head.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\nscheduler = CosineScheduler(epochs, lr, warmup_length)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T15:31:03.558739Z","iopub.execute_input":"2024-09-18T15:31:03.559360Z","iopub.status.idle":"2024-09-18T15:31:04.014712Z","shell.execute_reply.started":"2024-09-18T15:31:03.559308Z","shell.execute_reply":"2024-09-18T15:31:04.013885Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## 分類器の学習（並列処理）","metadata":{}},{"cell_type":"code","source":"# DataParallelを使用して複数GPUで並列処理\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = torch.nn.DataParallel(model)\n\n# モデルをデバイスに送る(必要！)\nmodel = model.to(device)\nfor epoch in range(epochs):\n    new_lr = scheduler(epoch)\n    set_lr(new_lr, optimizer)\n\n    total_train_loss = 0.\n    total_train_acc = 0.\n    total_valid_loss = 0.\n    total_valid_acc = 0.\n    for x, t in dataloader_train:\n        step_count += 1\n        x, t = x.to(device), t.to(device)\n        pred = model(x)\n\n        train_loss = criterion(pred, t)\n        train_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n\n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n\n        total_train_loss += train_loss.item()\n        total_train_acc += train_acc\n\n    with torch.no_grad():\n        for x, t in dataloader_valid:\n            x, t = x.to(device), t.to(device)\n            pred = model(x)\n\n            valid_loss = criterion(pred, t)\n            valid_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n\n            total_valid_loss += valid_loss.item()\n            total_valid_acc += valid_acc\n\n    print(f\"Epoch[{epoch+1} / {epochs}]\",\n          f\"Train Loss: {total_train_loss/len(dataloader_train):.4f}\",\n          f\"Train Acc.: {total_train_acc/len(dataloader_train):.4f}\",\n          f\"Valid Loss: {total_valid_loss/len(dataloader_valid):.4f}\",\n          f\"Valid Acc.: {total_valid_acc/len(dataloader_valid):.4f}\")\n","metadata":{"id":"p5T6QiHwyTRj","execution":{"iopub.status.busy":"2024-09-18T15:31:07.004067Z","iopub.execute_input":"2024-09-18T15:31:07.004776Z","iopub.status.idle":"2024-09-18T15:46:40.159185Z","shell.execute_reply.started":"2024-09-18T15:31:07.004737Z","shell.execute_reply":"2024-09-18T15:46:40.158208Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Using 2 GPUs!\nEpoch[1 / 10] Train Loss: 2.2739 Train Acc.: 0.1507 Valid Loss: 2.2278 Valid Acc.: 0.2026\nEpoch[2 / 10] Train Loss: 2.1841 Train Acc.: 0.2047 Valid Loss: 2.1578 Valid Acc.: 0.2073\nEpoch[3 / 10] Train Loss: 2.1281 Train Acc.: 0.2228 Valid Loss: 2.1043 Valid Acc.: 0.2344\nEpoch[4 / 10] Train Loss: 2.0892 Train Acc.: 0.2335 Valid Loss: 2.0758 Valid Acc.: 0.2249\nEpoch[5 / 10] Train Loss: 2.0630 Train Acc.: 0.2396 Valid Loss: 2.0513 Valid Acc.: 0.2451\nEpoch[6 / 10] Train Loss: 2.0471 Train Acc.: 0.2440 Valid Loss: 2.0425 Valid Acc.: 0.2314\nEpoch[7 / 10] Train Loss: 2.0359 Train Acc.: 0.2465 Valid Loss: 2.0352 Valid Acc.: 0.2386\nEpoch[8 / 10] Train Loss: 2.0289 Train Acc.: 0.2501 Valid Loss: 2.0220 Valid Acc.: 0.2489\nEpoch[9 / 10] Train Loss: 2.0232 Train Acc.: 0.2577 Valid Loss: 2.0215 Valid Acc.: 0.2382\nEpoch[10 / 10] Train Loss: 2.0211 Train Acc.: 0.2572 Valid Loss: 2.0217 Valid Acc.: 0.2451\n","output_type":"stream"}]},{"cell_type":"code","source":"model_Vit_path=\"/kaggle/working/ViT_supervised_params.pth\"\ntorch.save(model.state_dict(), model_Vit_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T15:47:07.004949Z","iopub.execute_input":"2024-09-18T15:47:07.005878Z","iopub.status.idle":"2024-09-18T15:47:07.055469Z","shell.execute_reply.started":"2024-09-18T15:47:07.005836Z","shell.execute_reply":"2024-09-18T15:47:07.054536Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nt_pred = []\nfor x in dataloader_test:\n    x = x.to(device)\n    y = model(x)\n\n    # モデルの出力を予測値のスカラーに変換\n    pred = y.argmax(1).tolist()\n    t_pred.extend(pred)\n\nsubmission = pd.Series(t_pred, name='label')\nsubmission_path=\"/kaggle/working/submission_pred.csv\"\nsubmission.to_csv(submission_path, header=True, index_label='id')","metadata":{"id":"72n19Q_RyqWl","execution":{"iopub.status.busy":"2024-09-18T15:47:09.990171Z","iopub.execute_input":"2024-09-18T15:47:09.990571Z","iopub.status.idle":"2024-09-18T15:47:18.155709Z","shell.execute_reply.started":"2024-09-18T15:47:09.990534Z","shell.execute_reply":"2024-09-18T15:47:18.154704Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"NwKJ8IBcy5TG"},"execution_count":null,"outputs":[]}]}