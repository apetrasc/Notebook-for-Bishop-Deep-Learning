{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序論\n",
    "Transformerは深層学習の発展の中で最も重要なモデルの一つである。これらはAttention（注意機構）と呼ばれる仕組みに基づいて動作しており、ネットワークそのものが異なる入力に対し異なる重みづけを行うことを可能とする一方で、重み係数に関しては入力に依存するということが可能となっている。結果、系列データであったり、その他の形式のデータにまつわる帰納バイアス（=inductive bias）をとらえる能力を持つ。  \n",
    "これらのモデルはtransformersと呼ばれるが、その理由は何らかの表現空間(=representation space)にあるひとつのまとまりを成すベクトルを、同じ次元のまま別の空間へと「変換」することに由来をもつ。これらの変換の目的は、変換先の空間が、より豊かな特徴をもち下流の問題を解くのに適した空間となるということである。Transformerの入力は、構造化されていないまとまりのベクトルや順序づけられた配列、あるいは一般的な表現となっており、Transformerの応用先を広範なものにしている。  \n",
    "Transformerは元来自然言語処理（Natural Language Processing, NLP）の文脈で紹介されてきた。そして、これらはRNNをベースとしたState-of-the-artの記録を次々と塗り替えていった。Transformersは他の分野においても素晴らしい精度を誇ることが確かめられている。例えば、Vision TransformersがCNNの性能を凌駕することは度々起こり、文字や画像、音声、動画など様々な形式のデータを組み合わせたMultimodal Transformersは最高性能をもつDeep learningモデルの一種をなす。\n",
    "これらTransformersの一つの大きな利点の一つに、転移学習（=transfer learning）が効果的であるというものがある。あるTransformerモデルは、膨大なデータによって訓練されたあと、何らかのFine-tuningを行うことによって別のタスクへと特化させていくことができるのである。巨視スケールをもち複数の下流タスクへと転用可能なモデルは基盤モデル（=foundation model）と呼ばれる。さらに、Transformerは自己教師あり学習のような形でラベル付けされていないデータを用いて訓練することが可能であり、インターネット他様々な情報源を用いて訓練できるためこれは言語モデルに対し特に効果的であるとされている。スケール則(scaling hypothesis)によると、単純にパラメータ数の意味でモデルの規模を増大させ膨大なデータセットで訓練を行うことによって、モデルの改造なしで大幅な性能向上をもたらすことを主張している。さらに、Transformersは後述するようにGPUを用いて大規模な並列計算を行うことに適しており、$10^{12}$程度のパラメータを持つモデルも現実的な時間で訓練を行うことが可能となっている。これらモデルは非常に高い性能を持ち、世界初の汎用人工知能（＝Artificial general intelligence）の印として表現される性質を備えるに至った。  \n",
    "Transformerの設計は複数の構造が絡まりあって動いているため初心者にとっては複雑怪奇に思え、辟易するほどである。結果、一見すると複数の設計がいい加減なものに思えるかもしれない。この章では、一歩一歩Transformersの背景にある考えを細大もらさず理解可能な形で紹介し、複数の要素を設計する動機を与える。初めにTransformersの設計について紹介してから、(12-1)、自然言語処理での応用事例を紹介し、(12-2,-3)最後にその他の応用分野について紹介するという構造をとる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
